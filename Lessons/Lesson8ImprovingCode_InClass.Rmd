---
title: "Lesson 8 Improving Code"
author: "Karsten Maurer"
date: "9/27/2019"
output: 
  html_document:
    number_sections: true
    pandoc_args: [
      "--number-sections",
      "--number-offset=7"
      ]
editor_options: 
  chunk_output_type: console
---

# Simulation Study Efficiency

In our example code from Monday we implemented a number of functions to run a sumulation study on the comparison of stepwise and lasso variable selection methods. It all ran, but was a bit slow... and was a bit of a mess. Today we will talk about speeding it up and cleaning it up. 

```{r example_var_select, warning=FALSE, message=FALSE}
### Libraries we will need containing the functions required
library(tidyverse)
library(glmnet)
library(caret)
library(tictoc)
library(parallel)
# install_github('nathanvan/parallelsugar')
library(parallelsugar)

make_sim_data <- function(n=100, p=20, q=10, b=0.1, sd_y=1, sd_x=1){
  X <- sapply(1:p, function(i) rnorm(n, 0, sd=sd_x))
  colnames(X) <- paste0("x",1:p)
  beta = c(rep(b, q), rep(0, p-q))
  y = (X %*% beta)[,1] + rnorm(n,0, sd_y)
  sim_data <- data.frame(y,X)
  return(sim_data)
}

### Helper functions for running the experiment with variable selection

# function for choosing with stepwise and fitting a regression
step_var_mod <- function(df){
  step_selected <- step(lm(y ~ . , data=df), trace = FALSE)
  return(step_selected)
}

# function for choosing with lasso and fitting regression
lasso_var_mod <- function(df){
  cv.out <- cv.glmnet(x= as.matrix(x=df[,-which(names(df)=="y")]),
                      y=df$y, alpha=1,type.measure="deviance")
  lasso_mod <- glmnet(x= as.matrix(x=df[,-which(names(df)=="y")]),
                      y=df$y, alpha=1, lambda=cv.out$lambda.1se)
  lasso_vars <- names(lasso_mod$beta[,1])[which(lasso_mod$beta[,1] != 0)]
  if (length(lasso_vars) == 0) lasso_vars <- names(lasso_mod$beta[,1])[1]
  lasso_selected <- lm( formula(paste0("y ~ 1 + ",paste(lasso_vars, collapse = " + "))), data=df )
  return(lasso_selected)
}

# function for finding number of variables included
select_var_count <- function(lin_mod){
  length(coef(lin_mod))-1
}

# function for finding 10-fold cross validated RMSE
select_cv_rmse <- function(lin_mod){
  cv_result <- train(formula(lin_mod), 
        data = lin_mod$model,
        method="lm",
        trControl=trainControl(method="cv",number=10),
        tuneGrid=data.frame(intercept=TRUE))
  return(cv_result$results$RMSE)
}

run_trial <- function(selection_alg, df){
  start_time = Sys.time()
  tmp_mod <- selection_alg(df)
  end_time = Sys.time()
  return(data.frame(var_count = select_var_count(tmp_mod),
            rmse = select_cv_rmse(tmp_mod),
            time = difftime(end_time, start_time, units="secs")))
}

## make into a function of n_sims, n, p and q
sim_var_select <- function(n_sim=10, n=100, p=10, q=5, var_select_ftn=step_var_mod){
  results <- NULL
  for(i in 1:n_sim){
    sim_data <- make_sim_data(n=n, p=p,q=q)
    results <- rbind(results, run_trial(var_select_ftn, sim_data))
  }
  return(results)
}

sim_var_select(10, 500, 10, 5, var_select_ftn=step_var_mod)
```


## Running Code in Parallel

```{r parallelization, eval=FALSE}
## Possible parameter combinations
params <- expand.grid(n=c(50,100,200), p=c(20,25,30), q=c(5,10))

tic()
sim_results <- lapply(1:nrow(params), function(i){
        sim_var_select(n=params$n[i], p=params$p[i], q=params$q[i],
                       n_sim=3,var_select_ftn=step_var_mod )
       })
toc()

### Parallel Processing: 
## parallel package by Luke Tierney
# easy with lapply and mapply --> mclapply and mcmapply on Mac and Linux
# Hard on Windows which want to use seriel processing 
## parallelsugar package by Nathan VanHoudnos gives windows hacked version

```

## Code Profiling

```{r profiling_code, error=FALSE, eval=FALSE}
### Profiling code with Rprof utility function
utils::Rprof(tmp <- tempfile())
sim_var_select(n_sim=10, n=100, p=10, q=5, var_select_ftn=step_var_mod)
Rprof()
summaryRprof(tmp)
unlink(tmp)
```


